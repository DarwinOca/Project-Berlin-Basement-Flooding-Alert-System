{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def skip_cell():\n",
    "    \"\"\"Skip execution of the current cell when called.\"\"\"\n",
    "    display(Markdown(\"**⏭️ Skipped this cell**\"))\n",
    "    # raise SystemExit\n",
    "    return # Exits the function, does NOT stop notebook execution\n",
    "# --------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------\n",
    "\n",
    "GROUNDWATER_PATH = '../data/wasserportal/processed/gw_master_2022-01-01_2025-04-30.parquet'\n",
    "STATIONS_PATH = '../data/wasserportal/stations_groundwater.csv'\n",
    "PRECIP_ZARR_PATH = '../data/dwd/processed/radolan_berlin_2022-01-01_2025-04-30.zarr'\n",
    "\n",
    "STATION_ID = '9931'  # Change to your target station\n",
    "N_GW_LAGS = 4\n",
    "N_PRCP_LAGS = 4\n",
    "INCLUDE_PRCP_T_PLUS_1 = True\n",
    "SEASONALITY = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this section, we wi'll load the required datasets for groundwater level prediction:\n",
    "1. **Groundwater levels**: Historical measurements from monitoring stations\n",
    "2. **Precipitation data**: Radar-based precipitation data from DWD (German Weather Service)\n",
    "3. **Station metadata**: Geographic coordinates and station information\n",
    "\n",
    "Our goal is to predict groundwater levels using historical groundwater data and precipitation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------\n",
    "\n",
    "# Groundwater levels\n",
    "gw_df = pd.read_parquet(GROUNDWATER_PATH)\n",
    "display(gw_df.head())\n",
    "gw_df['date'] = pd.to_datetime(gw_df['date'])\n",
    "gw_series = gw_df[gw_df[\"station\"] == int(STATION_ID)].dropna()\n",
    "gw_series = gw_series.set_index('date')\n",
    "gw_series.index = pd.to_datetime(gw_series.index)\n",
    "\n",
    "# Timestamps\n",
    "dates = gw_series.index\n",
    "display(dates)\n",
    "\n",
    "# Load precipitation from zarr\n",
    "import xarray as xr\n",
    "\n",
    "precip_ds = xr.open_zarr(PRECIP_ZARR_PATH)\n",
    "precip_array = precip_ds['precipitation'].values  # shape: (time, 30, 30)\n",
    "\n",
    "# Load coordinates\n",
    "lats = precip_ds['lat'].values\n",
    "lons = precip_ds['lon'].values\n",
    "\n",
    "# Load station metadata\n",
    "stations_df = pd.read_csv(STATIONS_PATH)\n",
    "station = stations_df[stations_df['ID'] == int(STATION_ID)].iloc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Connect to the server , mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import datetime\n",
    "from sqlalchemy import create_engine, text, DateTime\n",
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Inside container: DB_HOST is set via docker-compose\n",
    "# On host: fallback to LOCAL_DB_HOST\n",
    "DB_HOST = os.getenv(\"DB_HOST\", os.getenv(\"LOCAL_DB_HOST\", \"localhost\"))\n",
    "DB_USER = os.getenv(\"DB_USER\", \"root\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"mysecretpassword\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"mydatabase\")\n",
    "\n",
    "# Build SQLAlchemy connection string\n",
    "DATABASE_URL = f\"mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\"\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT 1\"))\n",
    "        print(\"✅ Connected to MySQL, test query result:\", result.scalar())\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)\n",
    "\n",
    "print (engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Write data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append('../src/utils')\n",
    "\n",
    "import importlib\n",
    "import MySQL_write_append  # since you added ../utils to sys.path\n",
    "importlib.reload(MySQL_write_append)\n",
    "from MySQL_write_append import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_sql(gw_df,engine,\"gw_table\")\n",
    "write_df_to_sql(stations_df, engine, \"stations_meta\")\n",
    "write_precip_data(precip_ds, engine, table_name=\"precip_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Read back from database\n",
    "\n",
    "1. Data frames are straightforward, with read_sql\n",
    "2. 3D dataset, the DataFrame retreived from MySQL need to be transformed into zarr grid structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by reading back, gw_table should be identical to gw_df\n",
    "df_tmp = pd.read_sql(\"SELECT * FROM gw_table \", engine)\n",
    "df_tmp.set_index('date', inplace=True)\n",
    "display(df_tmp.head())\n",
    "df_tmp.equals(gw_df) # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by read back stantion meta data\n",
    "engine.dispose()\n",
    "df_stations = pd.read_sql(\"SELECT * FROM stations_meta\",engine)\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Here comes the 3D grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test read back precipitation data\n",
    "df_tmp2 = pd.read_sql(\"SELECT * FROM precip_df \", engine, parse_dates=['time'])\n",
    "\n",
    "tmp_ds = df_tmp2.set_index([\"time\", \"x\", \"y\"]).to_xarray()\n",
    "# Recreate xarray Dataset from DataFrame read from SQL\n",
    "# Reattach curvilinear lat/lon as coords (from stored values)\n",
    "lat_grid = df_tmp2.drop_duplicates(subset=[\"x\", \"y\"]).pivot(index=\"x\", columns=\"y\", values=\"lat\").values\n",
    "lon_grid = df_tmp2.drop_duplicates(subset=[\"x\", \"y\"]).pivot(index=\"x\", columns=\"y\", values=\"lon\").values\n",
    "\n",
    "tmp_ds3 = tmp_ds.assign_coords(\n",
    "    lat=((\"x\", \"y\"), lat_grid),\n",
    "    lon=((\"x\", \"y\"), lon_grid)\n",
    ")\n",
    "\n",
    "tmp_ds3 = tmp_ds3.drop_vars(['x', 'y'])\n",
    "\n",
    "# this should be similar to precip_ds\n",
    "print(tmp_ds3.all) # This will be the final zarr grid\n",
    "print(precip_ds.all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell\n",
    "should_skip = False\n",
    "if should_skip:\n",
    "    # raise SystemExit\n",
    "    skip_cell()\n",
    "else:\n",
    "    pass\n",
    "    # Any code below will run if \"should_skip\" is False\n",
    "    # Compare datasets ignoring the coordinates\n",
    "    print(tmp_ds3.all)\n",
    "    precip_ds[\"time\"] = (\"time\", pd.to_datetime(precip_ds[\"time\"].values).normalize()) # remove time component\n",
    "    print(xr.testing.assert_allclose(precip_ds, tmp_ds3))\n",
    "\n",
    "    # Direct difference of precipitation values\n",
    "    diff = (precip_ds[\"precipitation\"] - tmp_ds3[\"precipitation\"])\n",
    "\n",
    "    print(\"Max difference:\", float(diff.max()))\n",
    "    print(\"Min difference:\", float(diff.min()))\n",
    "\n",
    "    # Check if all values are exactly equal\n",
    "    print(\"All equal:\", bool((diff == 0).all())) ## almost zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
