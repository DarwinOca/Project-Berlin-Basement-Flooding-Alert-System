{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83cbb19",
   "metadata": {},
   "source": [
    "# XGBoost Model Notebook with SQL integration\n",
    "\n",
    "This notebook tests the multi-station model training functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e15267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('✅ Connected to MySQL, test query result:', 1)\n",
      "Tables:\n",
      "\n",
      "  Tables_in_mlops_database\n",
      "0                 gw_table\n",
      "1             precip_table\n",
      "2               pred_table\n",
      "3            stations_meta\n",
      "Stations:\n",
      "\n",
      "   station\n",
      "0        1\n",
      "1      100\n",
      "2    10035\n",
      "3    10038\n",
      "4      101\n",
      "5      102\n",
      "6      103\n",
      "7    10414\n",
      "8    10417\n",
      "9    10420\n",
      "Empty DataFrame\n",
      "Columns: [date, station, day_1, day_2, day_3, day_4, day_5, day_6, day_7]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "sys.path.append(os.path.abspath('../src/'))\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# import utils.model_utils as model_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importlib.reload(model_utils)\n",
    "from data.make_dataset import get_engine\n",
    "\n",
    "engine, _ = get_engine()\n",
    "print('Tables:\\n')\n",
    "print(pd.read_sql(\"SHOW TABLES\", engine))\n",
    "print('Stations:\\n')\n",
    "print(pd.read_sql(\"SELECT DISTINCT station FROM gw_table LIMIT 10\", engine))\n",
    "\n",
    "print(pd.read_sql(\"SELECT * FROM pred_table LIMIT 100\", engine))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05873d1e",
   "metadata": {},
   "source": [
    "## Test Multi-Station Model Training\n",
    "\n",
    "Let's test the updated functions that can handle multiple stations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8560d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multi-station training with stations: ['100']\n",
      "Date range: 2022-01-01 to 2025-04-30\n",
      "Test size: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Define test parameters\n",
    "station_list = ['100']\n",
    "#, ['100', '106', '115']  # Replace with actual station IDs\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2025-04-30'\n",
    "test_size = 0.2\n",
    "\n",
    "print(f\"Testing multi-station training with stations: {station_list}\")\n",
    "print(f\"Date range: {start_date} to {end_date}\")\n",
    "print(f\"Test size: {test_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7287c3",
   "metadata": {},
   "source": [
    "## Step 1: Load and Examine Data\n",
    "\n",
    "First, let's load the data for multiple stations and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c11e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data for multiple stations...\n",
      "('✅ Connected to MySQL, test query result:', 1)\n",
      "('✅ Connected to MySQL, test query result:', 1)\n",
      "\n",
      "Groundwater data shape: (1212, 2)\n",
      "Precipitation data shape: (1212, 2)\n",
      "\n",
      "Stations in groundwater data: [100]\n",
      "Stations in precipitation data: [100]\n",
      "\n",
      "Groundwater data sample:\n",
      "            value  station\n",
      "date                      \n",
      "2022-01-01  30.64      100\n",
      "2022-01-02  30.66      100\n",
      "2022-01-03  30.67      100\n",
      "2022-01-04  30.69      100\n",
      "2022-01-05  30.67      100\n",
      "\n",
      "Precipitation data sample:\n",
      "            precipitation  station\n",
      "date                              \n",
      "2022-01-01            6.5      100\n",
      "2022-01-02            1.6      100\n",
      "2022-01-03            5.2      100\n",
      "2022-01-04            1.9      100\n",
      "2022-01-05            0.8      100\n"
     ]
    }
   ],
   "source": [
    "# Load training data for multiple stations\n",
    "from models.utils import load_training_data\n",
    "\n",
    "print(\"Loading training data for multiple stations...\")\n",
    "gw_df, precip_df = load_training_data(station_list, start_date, end_date)\n",
    "\n",
    "print(f\"\\nGroundwater data shape: {gw_df.shape}\")\n",
    "print(f\"Precipitation data shape: {precip_df.shape}\")\n",
    "\n",
    "print(f\"\\nStations in groundwater data: {gw_df['station'].unique()}\")\n",
    "print(f\"Stations in precipitation data: {precip_df['station'].unique()}\")\n",
    "\n",
    "print(f\"\\nGroundwater data sample:\")\n",
    "print(gw_df.head())\n",
    "\n",
    "print(f\"\\nPrecipitation data sample:\")\n",
    "print(precip_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848659c",
   "metadata": {},
   "source": [
    "## Step 2: Build Features\n",
    "\n",
    "Now let's build the features from the multi-station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa33fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features from multi-station data...\n",
      "\n",
      "Feature matrix shape: (1201, 7)\n",
      "Target matrix shape: (1201, 7)\n",
      "\n",
      "Feature columns:\n",
      "['gw_lag_4', 'gw_lag_3', 'gw_lag_2', 'gw_lag_1', 'prcp_sum_14', 'prcp_sum_30', 'month']\n",
      "\n",
      "Feature matrix sample:\n",
      "    gw_lag_4  gw_lag_3  gw_lag_2  gw_lag_1  prcp_sum_14  prcp_sum_30  month\n",
      "0      30.64     30.66     30.67     30.69          NaN          NaN      1\n",
      "1      30.66     30.67     30.69     30.67          NaN          NaN      1\n",
      "2      30.67     30.69     30.67     30.61          NaN          NaN      1\n",
      "3      30.69     30.67     30.61     30.64          NaN          NaN      1\n",
      "4      30.67     30.61     30.64     30.64          NaN          NaN      1\n",
      "5      30.61     30.64     30.64     30.68          NaN          NaN      1\n",
      "6      30.64     30.64     30.68     30.60          NaN          NaN      1\n",
      "7      30.64     30.68     30.60     30.57          NaN          NaN      1\n",
      "8      30.68     30.60     30.57     30.59          NaN          NaN      1\n",
      "9      30.60     30.57     30.59     30.61         19.1          NaN      1\n",
      "10     30.57     30.59     30.61     30.62         12.6          NaN      1\n",
      "11     30.59     30.61     30.62     30.61         11.0          NaN      1\n",
      "12     30.61     30.62     30.61     30.64         14.3          NaN      1\n",
      "13     30.62     30.61     30.64     30.64         12.4          NaN      1\n",
      "14     30.61     30.64     30.64     30.59         11.6          NaN      1\n",
      "15     30.64     30.64     30.59     30.63         12.9          NaN      1\n",
      "16     30.64     30.59     30.63     30.65         15.6          NaN      1\n",
      "17     30.59     30.63     30.65     30.61         15.8          NaN      1\n",
      "18     30.63     30.65     30.61     30.60         16.2          NaN      1\n",
      "19     30.65     30.61     30.60     30.60         16.2          NaN      1\n",
      "20     30.61     30.60     30.60     30.60         15.9          NaN      1\n",
      "21     30.60     30.60     30.60     30.61         15.4          NaN      1\n",
      "22     30.60     30.60     30.61     30.63         16.0          NaN      1\n",
      "23     30.60     30.61     30.63     30.66         16.1          NaN      1\n",
      "24     30.61     30.63     30.66     30.62         23.3          NaN      1\n",
      "25     30.63     30.66     30.62     30.65         26.3         45.4      1\n",
      "26     30.66     30.62     30.65     30.69         17.8         38.9      1\n",
      "27     30.62     30.65     30.69     30.70         18.4         37.9      2\n",
      "28     30.65     30.69     30.70     30.65         24.3         38.6      2\n",
      "29     30.69     30.70     30.65     30.67         23.1         37.4      2\n",
      "\n",
      "Target matrix sample (first 5 rows, all prediction days):\n",
      "[[30.61 30.64 30.64 30.68 30.6  30.57 30.59]\n",
      " [30.64 30.64 30.68 30.6  30.57 30.59 30.61]\n",
      " [30.64 30.68 30.6  30.57 30.59 30.61 30.62]\n",
      " [30.68 30.6  30.57 30.59 30.61 30.62 30.61]\n",
      " [30.6  30.57 30.59 30.61 30.62 30.61 30.64]]\n"
     ]
    }
   ],
   "source": [
    "# Build features from the loaded data\n",
    "from features.build_features import build_features\n",
    "\n",
    "print(\"Building features from multi-station data...\")\n",
    "X, y = build_features(gw_df, precip_df)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "print(f\"\\nFeature matrix sample:\")\n",
    "print(X.head(30))\n",
    "\n",
    "print(f\"\\nTarget matrix sample (first 5 rows, all prediction days):\")\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb9dc03",
   "metadata": {},
   "source": [
    "## Step 3: Train Multi-Station Model\n",
    "\n",
    "Train a single model on data from all stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9818821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('✅ Connected to MySQL, test query result:', 1)\n",
      "('✅ Connected to MySQL, test query result:', 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m importlib.reload(models.train_model)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model, metrics, y_pred, y_test, results = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\src\\models\\train_model.py:31\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(station_ids, start_date, end_date, test_size)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(station_ids, start_date, end_date, test_size=\u001b[32m0\u001b[39m):\n\u001b[32m     27\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    Trains a model for multiple stations using groundwater and precipitation data.\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[33;03m    Tracks parameters, metrics, and model with MLflow.\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     gw_df, precip_df = \u001b[43mload_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Start MLFlow tracking\u001b[39;00m\n\u001b[32m     34\u001b[39m     mlflow.set_tracking_uri(\u001b[33m\"\u001b[39m\u001b[33mhttp://127.0.0.1:5000\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\src\\models\\utils.py:82\u001b[39m, in \u001b[36mload_training_data\u001b[39m\u001b[34m(station_ids, start_date, end_date)\u001b[39m\n\u001b[32m     79\u001b[39m all_precip_data = []\n\u001b[32m     80\u001b[39m engine, _ = get_engine()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstation_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstation_ids\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Load Groundwater levels for this station\u001b[39;49;00m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgw_df\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSELECT date, value FROM gw_table WHERE station = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstation_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AND date BETWEEN \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstart_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m AND \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mend_date\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgw_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import models.train_model\n",
    "\n",
    "importlib.reload(models.train_model)\n",
    "from models.train_model import train\n",
    "\n",
    "model, metrics, y_pred, y_test, results = train(100,\n",
    "                                                start_date,\n",
    "                                                end_date,\n",
    "                                                test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92877bc2",
   "metadata": {},
   "source": [
    "## Step 4: Test Prediction on Individual Station\n",
    "\n",
    "Test using the trained multi-station model to predict for a single station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26baeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing prediction for station 100\n",
      "Prediction date range: 2023-01-01 to 2023-01-31\n",
      "('✅ Connected to MySQL, test query result:', 1)\n",
      "\n",
      "==================================================\n",
      "MULTI-OUTPUT PREDICTION RESULTS\n",
      "==================================================\n",
      "   day_1: RMSE=0.032, MAE=0.026, R2=0.187\n",
      "   day_2: RMSE=0.043, MAE=0.035, R2=-0.504\n",
      "   day_3: RMSE=0.043, MAE=0.036, R2=-0.515\n",
      "   day_4: RMSE=0.039, MAE=0.032, R2=-0.190\n",
      "   day_5: RMSE=0.038, MAE=0.028, R2=-0.134\n",
      "   day_6: RMSE=0.039, MAE=0.031, R2=-0.196\n",
      "   day_7: RMSE=0.039, MAE=0.033, R2=-0.203\n"
     ]
    }
   ],
   "source": [
    "# Test prediction on a single station using the multi-station trained model\n",
    "import models.utils\n",
    "\n",
    "importlib.reload(models.utils)\n",
    "import models.predict_model\n",
    "\n",
    "importlib.reload(models.predict_model)\n",
    "\n",
    "from models.predict_model import predict_station\n",
    "from models.utils import load_model_from_registry\n",
    "\n",
    "from models.utils import load_model_from_registry, load_model_from_local_path\n",
    "\n",
    "# model = load_model_from_registry(\"my_model\", \"Production\")\n",
    "\n",
    "# model = load_model_from_local_path(\"./mlruns/0/models/m-57fec30aa1ad46aeaec956654f205a7f/artifacts/model\")\n",
    "# model = load_model_from_registry(\"my_model\", \"Production\")\n",
    "# predict_station(100, model, start_date, end_date)\n",
    "\n",
    "test_station = station_list[0]  # Use first station for testing\n",
    "pred_start_date = '2023-01-01'\n",
    "pred_end_date = '2023-01-31'\n",
    "\n",
    "print(f\"Testing prediction for station {test_station}\")\n",
    "print(f\"Prediction date range: {pred_start_date} to {pred_end_date}\")\n",
    "\n",
    "predictions = predict_station(test_station, model, pred_start_date,\n",
    "                              pred_end_date)\n",
    "\n",
    "# print(predictions)\n",
    "# # for day, preds in predictions.items():\n",
    "# #     print(\n",
    "# #         f\"{day}: {len(preds)} predictions, mean={np.mean(preds):.3f}, std={np.std(preds):.3f}\"\n",
    "# #     )\n",
    "# #     print(f\"  Sample predictions: {preds[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe32494",
   "metadata": {},
   "source": [
    "# Prediction from loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426fc91",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: '\\mlflow\\artifacts\\1\\models\\m-db51aeb9478a4788a684ba9ffc4eba58\\artifacts\\.'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m mlflow.set_tracking_uri(\u001b[33m\"\u001b[39m\u001b[33mhttp://127.0.0.1:5000\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m station_id =\u001b[32m100\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mload_model_from_registry\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_station_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstation_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m results = predict_station(\u001b[32m100\u001b[39m, model, start_date, end_date)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\src\\models\\utils.py:36\u001b[39m, in \u001b[36mload_model_from_registry\u001b[39m\u001b[34m(model_name, version)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03mLoads a model from the MLflow Model Registry.\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     35\u001b[39m model_uri = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43msklearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\.venv\\Lib\\site-packages\\mlflow\\sklearn\\__init__.py:652\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, dst_path)\u001b[39m\n\u001b[32m    617\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(model_uri, dst_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    618\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    619\u001b[39m \u001b[33;03m    Load a scikit-learn model from a local file or a run.\u001b[39;00m\n\u001b[32m    620\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m        predictions = sk_model.predict(pandas_df)\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m     local_model_path = \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     flavor_conf = _get_flavor_configuration(model_path=local_model_path, flavor_name=FLAVOR_NAME)\n\u001b[32m    654\u001b[39m     _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\.venv\\Lib\\site-packages\\mlflow\\tracking\\artifact_utils.py:119\u001b[39m, in \u001b[36m_download_artifact_from_uri\u001b[39m\u001b[34m(artifact_uri, output_path, lineage_header_info, tracking_uri)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, ModelsArtifactRepository):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m            \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlineage_header_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineage_header_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m repo.download_artifacts(artifact_path=artifact_path, dst_path=output_path)\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\.venv\\Lib\\site-packages\\mlflow\\store\\artifact\\models_artifact_repo.py:236\u001b[39m, in \u001b[36mModelsArtifactRepository.download_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path, lineage_header_info)\u001b[39m\n\u001b[32m    232\u001b[39m     model_path = \u001b[38;5;28mself\u001b[39m.repo.download_artifacts(\n\u001b[32m    233\u001b[39m         artifact_path, dst_path, lineage_header_info=lineage_header_info\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     model_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# NB: only add the registered model metadata iff the artifact path is at the root model\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# directory. For individual files or subdirectories within the model directory, do not\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# create the metadata file.\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(model_path) \u001b[38;5;129;01mand\u001b[39;00m MLMODEL_FILE_NAME \u001b[38;5;129;01min\u001b[39;00m os.listdir(model_path):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaebe\\Documents\\ML Engineering Bootcamp\\jul25_bmlops_flooding\\.venv\\Lib\\site-packages\\mlflow\\store\\artifact\\local_artifact_repo.py:92\u001b[39m, in \u001b[36mLocalArtifactRepository.download_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path)\u001b[39m\n\u001b[32m     90\u001b[39m local_artifact_path = os.path.join(\u001b[38;5;28mself\u001b[39m.artifact_dir, os.path.normpath(artifact_path))\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(local_artifact_path):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_artifact_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m os.path.abspath(local_artifact_path)\n",
      "\u001b[31mOSError\u001b[39m: No such file or directory: '\\mlflow\\artifacts\\1\\models\\m-db51aeb9478a4788a684ba9ffc4eba58\\artifacts\\.'"
     ]
    }
   ],
   "source": [
    "from models.utils import load_model_from_registry\n",
    "from models.predict_model import predict_station\n",
    "import mlflow\n",
    "import models.predict_model\n",
    "\n",
    "importlib.reload(models.predict_model)\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "\n",
    "station_id = 100\n",
    "model = load_model_from_registry(f\"model_station_{station_id}\")\n",
    "results = predict_station(100, model, start_date, end_date)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
