{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import datetime\n",
    "from sqlalchemy import create_engine, text, DateTime\n",
    "import mysql.connector\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/utils')\n",
    "from src.utils import data_loading_dwd, data_loading_wasserportal, helpers\n",
    "\n",
    "importlib.reload(data_loading_wasserportal)\n",
    "importlib.reload(data_loading_dwd)\n",
    "# importlib.reload(radolan_handler)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Inside container: DB_HOST is set via docker-compose\n",
    "# On host: fallback to LOCAL_DB_HOST\n",
    "DB_HOST = os.getenv(\"DB_HOST\", os.getenv(\"LOCAL_DB_HOST\", \"localhost\"))\n",
    "DB_USER = os.getenv(\"DB_USER\", \"root\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"mysecretpassword\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"mydatabase\")\n",
    "\n",
    "# Build SQLAlchemy connection string\n",
    "DATABASE_URL = f\"mysql+mysqlconnector://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}\"\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True)\n",
    "\n",
    "def skip_cell():\n",
    "    \"\"\"Skip execution of the current cell when called.\"\"\"\n",
    "    display(Markdown(\"**⏭️ Skipped this cell**\"))\n",
    "    # raise SystemExit\n",
    "    return # Exits the function, does NOT stop notebook execution\n",
    "# --------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------\n",
    "\n",
    "date_start = '2022-01-01'\n",
    "date_end = '2025-04-30'\n",
    "\n",
    "\n",
    "GROUNDWATER_PATH = '../data/wasserportal/processed/gw_data_' + date_start + '_' + date_end + '.parquet'\n",
    "STATIONS_PATH = '../data/wasserportal/stations_groundwater.csv'\n",
    "PRECIP_ZARR_PATH = '../data/dwd/processed/radolan_berlin_' + date_start + '_' + date_end + '.zarr'\n",
    "\n",
    "# Change to your target station\n",
    "STATION_ID = '9931'\n",
    "# STATION_ID = '100'\n",
    "N_GW_LAGS = 4\n",
    "N_PRCP_LAGS = 4\n",
    "INCLUDE_PRCP_T_PLUS_1 = True\n",
    "SEASONALITY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_groundwater = {\n",
    "    'thema': 'gws',\n",
    "    'exportthema': 'gw',\n",
    "    'sreihe': 'ew',\n",
    "    \"anzeige\": \"d\",\n",
    "    \"smode\": \"c\"\n",
    "}\n",
    "\n",
    "data_loading_wasserportal.get_multi_station_data(helpers.convert_date_format(date_start),\n",
    "                                                 helpers.convert_date_format(date_end),\n",
    "                                                 config_groundwater,\n",
    "                                                 num_stations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep raw files (recommended for production)\n",
    "results = data_loading_dwd.import_radolan_recent(date_start,\n",
    "                                                 date_end,\n",
    "                                                 '../data/dwd/',\n",
    "                                                 keep_raw=True)\n",
    "# Extract only noon files (12:50)\n",
    "results = data_loading_dwd.import_radolan_historical(date_start,\n",
    "                                                     date_end,\n",
    "                                                     '../data/dwd/',\n",
    "                                                     time_to_keep=1250,\n",
    "                                                     keep_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with Berlin bounds\n",
    "config = {\n",
    "    'bounds': {\n",
    "        'min_lat': 52.4,\n",
    "        'max_lat': 52.65,\n",
    "        'min_lon': 13.15,\n",
    "        'max_lon': 13.6\n",
    "    },\n",
    "    'date_range': {\n",
    "        'start_date': date_start,\n",
    "        'end_date': date_end\n",
    "    },\n",
    "    'region_name': 'berlin',\n",
    "    'data_directory': '../data/dwd/extracted',\n",
    "    'output_directory': '../data/dwd/processed'\n",
    "}\n",
    "\n",
    "data_processing_dwd.create_radolan_timeseries(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this section, we wi'll load the required datasets for groundwater level prediction:\n",
    "1. **Groundwater levels**: Historical measurements from monitoring stations\n",
    "2. **Precipitation data**: Radar-based precipitation data from DWD (German Weather Service)\n",
    "3. **Station metadata**: Geographic coordinates and station information\n",
    "\n",
    "Our goal is to predict groundwater levels using historical groundwater data and precipitation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# LOAD DATA\n",
    "# --------------------------\n",
    "\n",
    "# Groundwater levels\n",
    "gw_df = pd.read_parquet(GROUNDWATER_PATH)\n",
    "gw_series = gw_df[f\"value_{STATION_ID}\"].dropna()\n",
    "gw_series.index = pd.to_datetime(gw_series.index)\n",
    "gw_df.index = pd.to_datetime(gw_df.index)\n",
    "\n",
    "# Timestamps\n",
    "dates = gw_series.index\n",
    "display(dates)\n",
    "\n",
    "# Load precipitation from zarr\n",
    "import xarray as xr\n",
    "\n",
    "precip_ds = xr.open_zarr(PRECIP_ZARR_PATH)\n",
    "precip_array = precip_ds['precipitation'].values  # shape: (time, 30, 30)\n",
    "\n",
    "# Load coordinates\n",
    "lats = precip_ds['lat'].values\n",
    "lons = precip_ds['lon'].values\n",
    "\n",
    "# Load station metadata\n",
    "stations_df = pd.read_csv(STATIONS_PATH)\n",
    "station = stations_df[stations_df['ID'] == int(STATION_ID)].iloc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Connect to the server , mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT 1\"))\n",
    "        print(\"✅ Connected to MySQL, test query result:\", result.scalar())\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)\n",
    "\n",
    "print (engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Write data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write table : gw_df, Groundwater data\n",
    "# gw_df.to_sql('groundwater_data', con=engine, if_exists='replace', index=True)\n",
    "gw_df.to_sql(\"gw_table\", engine, if_exists=\"replace\", index=True, index_label=\"date\",dtype={\"date\": DateTime()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT * FROM gw_table LIMIT 5\"))\n",
    "    for row in result:\n",
    "        print(row)\n",
    "\n",
    "# Test by reading back, gw_df should be identical to gw_df\n",
    "df_tmp = pd.read_sql(\"SELECT * FROM gw_table \", engine)\n",
    "df_tmp.set_index('date', inplace=True)\n",
    "display(df_tmp.head())\n",
    "df_tmp.equals(gw_df) # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write table : precip_data, Precipitation data\n",
    "\n",
    "# Need to transform precip_ds to a DataFrame first\n",
    "\n",
    "# display(precip_ds)\n",
    "# Convert the dataset into a pandas DataFrame\n",
    "tmp_df = precip_ds.to_dataframe().reset_index()\n",
    "tmp_df.time = tmp_df.time.dt.date  # keep only date part\n",
    "# tmp_df = tmp_df[['time', 'lat', 'lon', 'precipitation']]\n",
    "tmp_df = tmp_df[['time', 'x', 'y', 'lat', 'lon', 'precipitation']]\n",
    "# tmp_df = tmp_df.set_index('time')\n",
    "# tmp_df = tmp_df.sort_index()\n",
    "print(tmp_df.shape)\n",
    "display(tmp_df.head())\n",
    "\n",
    "# Always create a fresh connection\n",
    "engine.dispose()\n",
    "tmp_df.to_sql('precip_df', engine, if_exists='replace', index=False, chunksize=10000, method=\"multi\" )\n",
    "\n",
    "# write in 10k row batches # sends multiple rows per INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by reading back, tmp_df should be identical to precip_df in the database\n",
    "df_tmp2 = pd.read_sql(\"SELECT * FROM precip_df \", engine, parse_dates=['time'])\n",
    "\n",
    "tmp_ds = df_tmp2.set_index([\"time\", \"x\", \"y\"]).to_xarray()\n",
    "\n",
    "# Attach original coordinates\n",
    "tmp_ds2 = tmp_ds.assign_coords(\n",
    "    lat=((\"x\",\"y\"), precip_ds.lat.values),\n",
    "    lon=((\"x\",\"y\"), precip_ds.lon.values)\n",
    ")\n",
    "\n",
    "# this should be similar to precip_ds\n",
    "print(tmp_ds2.all)\n",
    "\n",
    "##############################################################################\n",
    "# Recreate xarray Dataset from DataFrame read from SQL\n",
    "# Reattach curvilinear lat/lon as coords (from stored values)\n",
    "lat_grid = df_tmp2.drop_duplicates(subset=[\"x\", \"y\"]).pivot(index=\"x\", columns=\"y\", values=\"lat\").values\n",
    "lon_grid = df_tmp2.drop_duplicates(subset=[\"x\", \"y\"]).pivot(index=\"x\", columns=\"y\", values=\"lon\").values\n",
    "\n",
    "tmp_ds3 = tmp_ds.assign_coords(\n",
    "    lat=((\"x\", \"y\"), lat_grid),\n",
    "    lon=((\"x\", \"y\"), lon_grid)\n",
    ")\n",
    "# this should be similar to precip_ds\n",
    "print(tmp_ds3.all)\n",
    "\n",
    "print(xr.testing.assert_allclose(tmp_ds2, tmp_ds3))\n",
    "tmp_ds3 = tmp_ds3.drop_vars(['x', 'y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell\n",
    "should_skip = True\n",
    "if should_skip:\n",
    "    # raise SystemExit\n",
    "    skip_cell()\n",
    "else:\n",
    "    pass\n",
    "    # Any code below will run if should_skip is False\n",
    "    print(\"This will run if should_skip is False\")\n",
    "    # Compare datasets ignoring the coordinates\n",
    "    print(tmp_ds3.all)\n",
    "    precip_ds[\"time\"] = (\"time\", pd.to_datetime(precip_ds[\"time\"].values).normalize()) # remove time component\n",
    "    print(xr.testing.assert_allclose(precip_ds, tmp_ds3))\n",
    "\n",
    "    # Direct difference of precipitation values\n",
    "    diff = (precip_ds[\"precipitation\"] - tmp_ds3[\"precipitation\"])\n",
    "\n",
    "    print(\"Max difference:\", float(diff.max()))\n",
    "    print(\"Min difference:\", float(diff.min()))\n",
    "\n",
    "    # Check if all values are exactly equal\n",
    "    print(\"All equal:\", bool((diff == 0).all()))\n",
    "\n",
    "\n",
    "# Clean up, all read-back variables, release memory\n",
    "del df_tmp, df_tmp2, tmp_ds, tmp_ds2, tmp_ds3, lat_grid, lon_grid, tmp_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write station metadata\n",
    "print(stations_df.head())\n",
    "\n",
    "engine.dispose()\n",
    "stations_df.to_sql('stations_meta', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# no need to read back, stations_df is small and easy to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## load new data and append to existing tables, mainly for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "### load DWD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_new_data\n",
    "importlib.reload(load_new_data)\n",
    "\n",
    "from load_new_data import *\n",
    "\n",
    "# Usage examples:\n",
    "# These two variable need to be given by the user\n",
    "start_date = '2025-07-01'\n",
    "end_date = '2025-07-31'\n",
    "\n",
    "load_new_data_from_dwd(start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### load Groud water station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# These two variable need to be given by the user\n",
    "start_date = '01.06.2025'\n",
    "end_date = '30.06.2025'\n",
    "load_new_data_from_wasserportal(start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flood-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
